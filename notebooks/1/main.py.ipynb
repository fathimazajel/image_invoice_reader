{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bda7f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total .jpg files: 735\n",
      "Total annotated .txt files: 0\n",
      "Total json .txt files: 876\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Initialize counts\n",
    "jpg_count = 0\n",
    "annotated_count = 0\n",
    "json_count = 0\n",
    "\n",
    "# Define the folder path\n",
    "folder_path = \"/Users/fathimazajel/Desktop/image_invoice_processing_data/SROIE2019/0325updated.task2train(626p)\"\n",
    "\n",
    "# Iterate over files in the directory\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".jpg\"):\n",
    "        jpg_count += 1\n",
    "    elif filename.endswith(\".txt\"):\n",
    "        with open(os.path.join(folder_path, filename), \"r\") as f:\n",
    "            first_char = f.read(1)  # Read the first character\n",
    "\n",
    "            if first_char.isdigit():\n",
    "                annotated_count += 1\n",
    "            elif first_char == '{':\n",
    "                json_count += 1\n",
    "\n",
    "# Display the counts\n",
    "print(f\"Total .jpg files: {jpg_count}\")\n",
    "print(f\"Total annotated .txt files: {annotated_count}\")\n",
    "print(f\"Total json .txt files: {json_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8da68c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total .jpg files: 712\n",
      "Total annotated .txt files: 835\n",
      "Total json .txt files: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Initialize counts\n",
    "jpg_count = 0\n",
    "annotated_count = 0\n",
    "json_count = 0\n",
    "\n",
    "# Define the folder path\n",
    "folder_path = \"/Users/fathimazajel/Desktop/image_invoice_processing_data/SROIE2019/0325updated.task1train(626p)\"\n",
    "\n",
    "# Iterate over files in the directory\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".jpg\"):\n",
    "        jpg_count += 1\n",
    "    elif filename.endswith(\".txt\"):\n",
    "        with open(os.path.join(folder_path, filename), \"r\") as f:\n",
    "            first_char = f.read(1)  # Read the first character\n",
    "\n",
    "            if first_char.isdigit():\n",
    "                annotated_count += 1\n",
    "            elif first_char == '{':\n",
    "                json_count += 1\n",
    "\n",
    "# Display the counts\n",
    "print(f\"Total .jpg files: {jpg_count}\")\n",
    "print(f\"Total annotated .txt files: {annotated_count}\")\n",
    "print(f\"Total json .txt files: {json_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbd528a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files only in Folder 1:\n",
      "X51005361907(1).jpg\n",
      "X51005361950(1).jpg\n",
      "X51005365179(1).jpg\n",
      "X51005442376(1).jpg\n",
      "X51005442378(1).jpg\n",
      "X51005442379(1).jpg\n",
      "X51005442383(1).jpg\n",
      "X51005442384(1).jpg\n",
      "X51005568911(1).jpg\n",
      "X51005568913(1).jpg\n",
      "X51005568914(1).jpg\n",
      "X51005577192(1).jpg\n",
      "X51005605284(3).jpg\n",
      "X51005605284(4).jpg\n",
      "X51005605285(3).jpg\n",
      "X51005605286(2).jpg\n",
      "X51005605286(3).jpg\n",
      "X51005663324(1).jpg\n",
      "X51005675095(1).jpg\n",
      "X51005676536(1).jpg\n",
      "X51005676538(1).jpg\n",
      "X51005676539(1).jpg\n",
      "X51005677331(1).jpg\n",
      "X51005677332(1).jpg\n",
      "X51005677334(1).jpg\n",
      "X51005677334(2).jpg\n",
      "X51005677335(1).jpg\n",
      "X51005677339(1).jpg\n",
      "X51005677339(2).jpg\n",
      "X51005685355(2).jpg\n",
      "X51005685357(2).jpg\n",
      "X51005705722(1).jpg\n",
      "X51006311780(1).jpg\n",
      "X51006329395(2).jpg\n",
      "X51006334699(1).jpg\n",
      "X51006334699(2).jpg\n",
      "X51006334741(1).jpg\n",
      "X51006334741(2).jpg\n",
      "X51006334741(3).jpg\n",
      "X51006334926(1).jpg\n",
      "X51006334926(2).jpg\n",
      "X51006334926(3).jpg\n",
      "X51006335314(1).jpg\n",
      "X51006335314(2).jpg\n",
      "X51006335314(3).jpg\n",
      "X51006335547(1).jpg\n",
      "X51006335547(2).jpg\n",
      "X51006335547(3).jpg\n",
      "X51007103681(1).jpg\n",
      "X51007103692(1).jpg\n",
      "X51007225442(1).jpg\n",
      "X51007228448(1).jpg\n",
      "X51007231336(1).jpg\n",
      "X51007339114(1).jpg\n",
      "X51007339117(1).jpg\n",
      "X51007339118(1).jpg\n",
      "X51007339647(1).jpg\n",
      "\n",
      "Files only in Folder 2:\n",
      "X51005301659(1).jpg\n",
      "X51005301659(2).jpg\n",
      "X51005301659(3).jpg\n",
      "X51005301659(4).jpg\n",
      "X51005301661(1).jpg\n",
      "X51005301661(2).jpg\n",
      "X51005301667(1).jpg\n",
      "X51005301667(2).jpg\n",
      "X51005303661(1).jpg\n",
      "X51005303661(2).jpg\n",
      "X51005303661(3).jpg\n",
      "X51005303661(4).jpg\n",
      "X51005306399(1).jpg\n",
      "X51005306399(2).jpg\n",
      "X51005306399(3).jpg\n",
      "X51005306399(4).jpg\n",
      "X51005337872(1).jpg\n",
      "X51005361883(1).jpg\n",
      "X51005361895(1).jpg\n",
      "X51005361897(1).jpg\n",
      "X51005361898(1).jpg\n",
      "X51005447860(1).jpg\n",
      "X51005447861(1).jpg\n",
      "X51005453729(1).jpg\n",
      "X51005453801(1).jpg\n",
      "X51005453802(1).jpg\n",
      "X51005587267(1).jpg\n",
      "X51005663277(1).jpg\n",
      "X51005663279(1).jpg\n",
      "X51005663280(1).jpg\n",
      "X51005676540(1).jpg\n",
      "X51005676541(1).jpg\n",
      "X51005676543(1).jpg\n",
      "X51005676546(1).jpg\n",
      "X51005676546(2).jpg\n",
      "X51005676547(1).jpg\n",
      "X51005676547(2).jpg\n",
      "X51005676549(1).jpg\n",
      "X51005676549(2).jpg\n",
      "X51005677328(1).jpg\n",
      "X51005677328(2).jpg\n",
      "X51005677329(1).jpg\n",
      "X51005705804(1).jpg\n",
      "X51005711401(1).jpg\n",
      "X51005711403(1).jpg\n",
      "X51005711404(1).jpg\n",
      "X51005711454(1).jpg\n",
      "X51005712017(1).jpg\n",
      "X51005712039(1).jpg\n",
      "X51005715010(1).jpg\n",
      "X51005715451(1).jpg\n",
      "X51005717526(1).jpg\n",
      "X51005719912(1).jpg\n",
      "X51005719914(1).jpg\n",
      "X51005719917(1).jpg\n",
      "X51005722668(1).jpg\n",
      "X51005722699(1).jpg\n",
      "X51005757294(1).jpg\n",
      "X51005757304(1).jpg\n",
      "X51005757323(1).jpg\n",
      "X51005757324(1).jpg\n",
      "X51005757346(1).jpg\n",
      "X51005806702(1).jpg\n",
      "X51005806716(1).jpg\n",
      "X51006556838(1).jpg\n",
      "X51006556839(1).jpg\n",
      "X51006556840(1).jpg\n",
      "X51006556841(1).jpg\n",
      "X51006557185(1).jpg\n",
      "X51006557193(1).jpg\n",
      "X51006557195(1).jpg\n",
      "X51007339112(1).jpg\n",
      "X51007339157(1).jpg\n",
      "X51007339158(1).jpg\n",
      "X51007339166(1).jpg\n",
      "X51007339166(2).jpg\n",
      "X51007339639(1).jpg\n",
      "X51007339642(1).jpg\n",
      "X51007339643(1).jpg\n",
      "X51007339653(1).jpg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the folder paths\n",
    "folder_path1 = \"/Users/fathimazajel/Desktop/image_invoice_processing_data/SROIE2019/0325updated.task1train(626p)\"\n",
    "folder_path2 = \"/Users/fathimazajel/Desktop/image_invoice_processing_data/SROIE2019/0325updated.task2train(626p)\"\n",
    "\n",
    "# Get list of .jpg files in each directory\n",
    "jpg_files1 = {f for f in os.listdir(folder_path1) if f.endswith('.jpg')}\n",
    "jpg_files2 = {f for f in os.listdir(folder_path2) if f.endswith('.jpg')}\n",
    "\n",
    "# Find files that are in folder 1 but not in folder 2 and vice-versa\n",
    "only_in_folder1 = jpg_files1 - jpg_files2\n",
    "only_in_folder2 = jpg_files2 - jpg_files1\n",
    "\n",
    "# Display results\n",
    "if only_in_folder1:\n",
    "    print(\"Files only in Folder 1:\")\n",
    "    for f in sorted(only_in_folder1):\n",
    "        print(f)\n",
    "\n",
    "if only_in_folder2:\n",
    "    print(\"\\nFiles only in Folder 2:\")\n",
    "    for f in sorted(only_in_folder2):\n",
    "        print(f)\n",
    "\n",
    "if not only_in_folder1 and not only_in_folder2:\n",
    "    print(\"Both folders have matching .jpg files!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fffbd36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of unique files in Folder 1: 0\n",
      "Count of unique files in Folder 2: 0\n",
      "Both folders have matching .jpg files!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "# Function to sanitize filename\n",
    "def sanitize_filename(filename):\n",
    "    return re.sub(r'\\(.*?\\)', '', filename)\n",
    "\n",
    "# Define the folder paths\n",
    "folder_path1 = \"/Users/fathimazajel/Desktop/image_invoice_processing_data/SROIE2019/0325updated.task1train(626p)\"\n",
    "folder_path2 = \"/Users/fathimazajel/Desktop/image_invoice_processing_data/SROIE2019/0325updated.task2train(626p)\"\n",
    "\n",
    "# Get list of sanitized .jpg filenames in each directory\n",
    "jpg_files1 = {sanitize_filename(f) for f in os.listdir(folder_path1) if f.endswith('.jpg')}\n",
    "jpg_files2 = {sanitize_filename(f) for f in os.listdir(folder_path2) if f.endswith('.jpg')}\n",
    "\n",
    "# Find files that are in folder 1 but not in folder 2 and vice-versa\n",
    "only_in_folder1 = jpg_files1 - jpg_files2\n",
    "only_in_folder2 = jpg_files2 - jpg_files1\n",
    "\n",
    "# Display results\n",
    "print(f\"Count of unique files in Folder 1: {len(only_in_folder1)}\")\n",
    "print(f\"Count of unique files in Folder 2: {len(only_in_folder2)}\")\n",
    "\n",
    "if only_in_folder1:\n",
    "    print(\"\\nUnique files in Folder 1:\")\n",
    "    for f in sorted(only_in_folder1):\n",
    "        print(f)\n",
    "\n",
    "if only_in_folder2:\n",
    "    print(\"\\nUnique files in Folder 2:\")\n",
    "    for f in sorted(only_in_folder2):\n",
    "        print(f)\n",
    "\n",
    "if not only_in_folder1 and not only_in_folder2:\n",
    "    print(\"Both folders have matching .jpg files!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f04ef9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total common .jpg files copied: 626\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil\n",
    "\n",
    "# Paths for the folders\n",
    "folder_path1 = \"/Users/fathimazajel/Desktop/image_invoice_processing_data/SROIE2019/0325updated.task1train(626p)\"\n",
    "folder_path2 = \"/Users/fathimazajel/Desktop/image_invoice_processing_data/SROIE2019/0325updated.task2train(626p)\"\n",
    "destination_folder = \"/Users/fathimazajel/Desktop/image_invoice_processing_data/SROIE2019/img\"\n",
    "\n",
    "# Exclude files that have \"()\" in their filename\n",
    "def exclude_files(filename):\n",
    "    return not bool(re.search(r'\\(.*?\\)', filename))\n",
    "\n",
    "# Gather .jpg filenames from both directories\n",
    "jpg_files1 = {f for f in os.listdir(folder_path1) if f.endswith('.jpg') and exclude_files(f)}\n",
    "jpg_files2 = {f for f in os.listdir(folder_path2) if f.endswith('.jpg') and exclude_files(f)}\n",
    "\n",
    "# Identify files that are common to both directories\n",
    "common_files = jpg_files1.intersection(jpg_files2)\n",
    "\n",
    "# Copy the common files to the destination folder\n",
    "for file in common_files:\n",
    "    shutil.copy2(os.path.join(folder_path1, file), destination_folder)\n",
    "\n",
    "# Print the result\n",
    "print(f\"Total common .jpg files copied: {len(common_files)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "396bcd91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total matching .txt files copied as .csv: 626\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Paths\n",
    "txt_folder_path = \"/Users/fathimazajel/Desktop/image_invoice_processing_data/SROIE2019/0325updated.task1train(626p)\"\n",
    "img_folder_path = \"/Users/fathimazajel/Desktop/image_invoice_processing_data/SROIE2019/img\"\n",
    "destination_folder = \"/Users/fathimazajel/Desktop/image_invoice_processing_data/SROIE2019/box\"\n",
    "\n",
    "# Gather .jpg basenames from img_folder_path\n",
    "jpg_files = {os.path.splitext(f)[0] for f in os.listdir(img_folder_path) if f.endswith('.jpg')}\n",
    "\n",
    "# Check which .txt files match the .jpg base names\n",
    "matching_files = [f for f in os.listdir(txt_folder_path) if f.endswith('.txt') and os.path.splitext(f)[0] in jpg_files]\n",
    "\n",
    "# Copy the matching files to the destination folder with .csv extension\n",
    "for file in matching_files:\n",
    "    shutil.copy2(os.path.join(txt_folder_path, file), os.path.join(destination_folder, os.path.splitext(file)[0] + '.csv'))\n",
    "\n",
    "# Print the result\n",
    "print(f\"Total matching .txt files copied as .csv: {len(matching_files)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebb321b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total matching .txt files copied as .json: 626\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Paths\n",
    "txt_folder_path = \"/Users/fathimazajel/Desktop/image_invoice_processing_data/SROIE2019/0325updated.task2train(626p)\"\n",
    "img_folder_path = \"/Users/fathimazajel/Desktop/image_invoice_processing_data/SROIE2019/img\"\n",
    "destination_folder = \"/Users/fathimazajel/Desktop/image_invoice_processing_data/SROIE2019/key\"\n",
    "\n",
    "# Ensure destination directory exists\n",
    "os.makedirs(destination_folder, exist_ok=True)\n",
    "\n",
    "# Gather .jpg basenames from img_folder_path\n",
    "jpg_files = {os.path.splitext(f)[0] for f in os.listdir(img_folder_path) if f.endswith('.jpg')}\n",
    "\n",
    "# Check which .txt files match the .jpg base names\n",
    "matching_files = [f for f in os.listdir(txt_folder_path) if f.endswith('.txt') and os.path.splitext(f)[0] in jpg_files]\n",
    "\n",
    "# Copy the matching files to the destination folder with .json extension\n",
    "for file in matching_files:\n",
    "    shutil.copy2(os.path.join(txt_folder_path, file), os.path.join(destination_folder, os.path.splitext(file)[0] + '.json'))\n",
    "\n",
    "# Print the result\n",
    "print(f\"Total matching .txt files copied as .json: {len(matching_files)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "691f9cce",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "8",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3620\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2131\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2140\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 8",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Extract and label data\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m df_csv\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m---> 35\u001b[0m     text_segment \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m]\u001b[49m)\u001b[38;5;241m.\u001b[39mstrip()  \u001b[38;5;66;03m# The ninth column contains the text\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m# Simple method to match and label text segments\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m text_segment \u001b[38;5;129;01min\u001b[39;00m json_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompany\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/series.py:958\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    955\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[1;32m    957\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m--> 958\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_hashable(key):\n\u001b[1;32m    961\u001b[0m     \u001b[38;5;66;03m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[1;32m    962\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    963\u001b[0m         \u001b[38;5;66;03m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/series.py:1069\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1066\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[1;32m   1068\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1069\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39m_get_values_for_loc(\u001b[38;5;28mself\u001b[39m, loc, label)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3623\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3624\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3625\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3626\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3627\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3628\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 8"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Paths to the directories\n",
    "csv_folder = '/Users/fathimazajel/Documents/GitHub/image_invoice_reader/data/box'\n",
    "json_folder = '/Users/fathimazajel/Documents/GitHub/image_invoice_reader/data/key'\n",
    "\n",
    "# List all files\n",
    "csv_files = glob.glob(os.path.join(csv_folder, '*.csv'))\n",
    "json_files = glob.glob(os.path.join(json_folder, '*.json'))\n",
    "\n",
    "# Create a dictionary of JSON files\n",
    "json_file_dict = {os.path.splitext(os.path.basename(jf))[0]: jf for jf in json_files}\n",
    "\n",
    "# Initialize lists to hold texts and labels\n",
    "texts = []\n",
    "labels = []\n",
    "\n",
    "# Process each CSV file\n",
    "for csv_file in csv_files:\n",
    "    csv_filename_without_extension = os.path.splitext(os.path.basename(csv_file))[0]\n",
    "    \n",
    "    json_file = json_file_dict.get(csv_filename_without_extension)\n",
    "    if json_file:\n",
    "        # Load data\n",
    "        df_csv = pd.read_csv(csv_file, header=None, sep=\"\\t\")  # Adjust parameters if needed\n",
    "        \n",
    "        with open(json_file, 'r') as file:\n",
    "            json_data = json.load(file)\n",
    "        \n",
    "        # Extract and label data\n",
    "        for index, row in df_csv.iterrows():\n",
    "            text_segment = str(row[8]).strip()  # The ninth column contains the text\n",
    "            \n",
    "            # Simple method to match and label text segments\n",
    "            if text_segment in json_data[\"company\"]:\n",
    "                labels.append(\"company\")\n",
    "            elif text_segment in json_data[\"date\"]:\n",
    "                labels.append(\"date\")\n",
    "            elif text_segment in json_data[\"address\"]:\n",
    "                labels.append(\"address\")\n",
    "            elif text_segment in json_data[\"total\"]:\n",
    "                labels.append(\"total\")\n",
    "            else:\n",
    "                labels.append(\"other\")\n",
    "            \n",
    "            texts.append(text_segment)\n",
    "            \n",
    "    else:\n",
    "        print(f\"No corresponding JSON file found for {csv_file}\")\n",
    "        \n",
    "        \n",
    "    # ... (previous code)\n",
    "\n",
    "# Extract and label data\n",
    "for index, row in df_csv.iterrows():\n",
    "    if len(row) > 8:  # Make sure the row has at least 9 columns\n",
    "        text_segment = str(row[8]).strip()  # The ninth column contains the text\n",
    "        \n",
    "        # Simple method to match and label text segments\n",
    "        if text_segment in json_data[\"company\"]:\n",
    "            labels.append(\"company\")\n",
    "        elif text_segment in json_data[\"date\"]:\n",
    "            labels.append(\"date\")\n",
    "        elif text_segment in json_data[\"address\"]:\n",
    "            labels.append(\"address\")\n",
    "        elif text_segment in json_data[\"total\"]:\n",
    "            labels.append(\"total\")\n",
    "        else:\n",
    "            labels.append(\"other\")\n",
    "        \n",
    "        texts.append(text_segment)\n",
    "    else:\n",
    "        print(f\"Row {index} in file {csv_file} does not have 9 columns.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f8557c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/fathimazajel/opt/anaconda3/lib/python3.9/site-packages (1.4.2)\n",
      "Requirement already satisfied: openpyxl in /Users/fathimazajel/opt/anaconda3/lib/python3.9/site-packages (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/fathimazajel/opt/anaconda3/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/fathimazajel/opt/anaconda3/lib/python3.9/site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /Users/fathimazajel/opt/anaconda3/lib/python3.9/site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: et-xmlfile in /Users/fathimazajel/opt/anaconda3/lib/python3.9/site-packages (from openpyxl) (1.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/fathimazajel/opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas openpyxl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8870001e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 626 .txt files to .xls in /Users/fathimazajel/Documents/GitHub/image_invoice_reader/data/box2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Source and destination directories\n",
    "source_dir = \"/Users/fathimazajel/Documents/GitHub/image_invoice_reader/data/box\"\n",
    "dest_dir = \"/Users/fathimazajel/Documents/GitHub/image_invoice_reader/data/box2\"\n",
    "\n",
    "# Ensure destination directory exists\n",
    "os.makedirs(dest_dir, exist_ok=True)\n",
    "\n",
    "# List all .txt files in source directory\n",
    "txt_files = [f for f in os.listdir(source_dir) if f.endswith('.txt')]\n",
    "\n",
    "for txt_file in txt_files:\n",
    "    # Build the corresponding .xls filename\n",
    "    xls_file = os.path.splitext(txt_file)[0] + \".xls\"\n",
    "    \n",
    "    # Read each line from the txt file, split the data, and collect in a list\n",
    "    data_list = []\n",
    "    with open(os.path.join(source_dir, txt_file), 'r', encoding='utf-8') as infile:\n",
    "        for line in infile:\n",
    "            # Split by comma and extract columns and text\n",
    "            columns = line.strip().split(\",\")[:8]\n",
    "            text = \",\".join(line.strip().split(\",\")[8:])\n",
    "            data_list.append(columns + [text])\n",
    "\n",
    "    # Convert the data list to a DataFrame and write to Excel\n",
    "    df = pd.DataFrame(data_list, columns=[\"Column1\", \"Column2\", \"Column3\", \"Column4\", \"Column5\", \"Column6\", \"Column7\", \"Column8\", \"Text\"])\n",
    "    df.to_excel(os.path.join(dest_dir, xls_file), index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Converted {len(txt_files)} .txt files to .xls in {dest_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c88ebd02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file '/Users/fathimazajel/Documents/GitHub/image_invoice_reader/data/box2/X00016469612.xls' has 44 rows and 9 columns.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to the .xls file\n",
    "xls_file_path = \"/Users/fathimazajel/Documents/GitHub/image_invoice_reader/data/box2/X00016469612.xls\"  # replace 'filename.xls' with the name of the file you want to check\n",
    "\n",
    "# Read the Excel file\n",
    "df = pd.read_excel(xls_file_path, engine='openpyxl')\n",
    "\n",
    "# Get the dimensions\n",
    "num_rows, num_cols = df.shape\n",
    "\n",
    "print(f\"The file '{xls_file_path}' has {num_rows} rows and {num_cols} columns.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8735196f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON file /Users/fathimazajel/Documents/GitHub/image_invoice_reader/data/key/X51005663280.json does not contain the 'address' key.\n",
      "{'company': 'T.A.S LEISURE SDN BHD', 'date': '30 DEC 17', 'total': '102.40'}\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "\n",
    "xls_folder = '/Users/fathimazajel/Documents/GitHub/image_invoice_reader/data/box2'\n",
    "json_folder = '/Users/fathimazajel/Documents/GitHub/image_invoice_reader/data/key'\n",
    "\n",
    "xls_files = glob.glob(os.path.join(xls_folder, '*.xls'))\n",
    "json_files = glob.glob(os.path.join(json_folder, '*.json'))\n",
    "\n",
    "json_file_dict = {os.path.splitext(os.path.basename(jf))[0]: jf for jf in json_files}\n",
    "\n",
    "texts = []\n",
    "labels = []\n",
    "\n",
    "for xls_file in xls_files:\n",
    "    xls_filename_without_extension = os.path.splitext(os.path.basename(xls_file))[0]\n",
    "    \n",
    "    json_file = json_file_dict.get(xls_filename_without_extension)\n",
    "    if json_file:\n",
    "        df_xls = pd.read_excel(xls_file, header=None)\n",
    "        \n",
    "        with open(json_file, 'r') as file:\n",
    "            json_data = json.load(file)\n",
    "        \n",
    "        # Check if 'address' is missing in the JSON data and print the content\n",
    "        if 'address' not in json_data:\n",
    "            print(f\"JSON file {json_file} does not contain the 'address' key.\")\n",
    "            print(json_data)\n",
    "            print(\"-\" * 50)  # Just for better visual separation\n",
    "        \n",
    "        # This assumes the last column in the xls file is the text\n",
    "        for _, row in df_xls.iterrows():\n",
    "            segment = str(row.iloc[-1])\n",
    "\n",
    "            # Basic matching for labels. This is very rudimentary and might need further refinement.\n",
    "            if 'company' in json_data and segment in json_data['company']:\n",
    "                label = \"company\"\n",
    "            elif 'date' in json_data and segment in json_data['date']:\n",
    "                label = \"date\"\n",
    "            elif 'address' in json_data and segment in json_data['address']:\n",
    "                label = \"address\"\n",
    "            elif 'total' in json_data and segment in json_data['total']:\n",
    "                label = \"total\"\n",
    "            else:\n",
    "                label = \"other\"\n",
    "\n",
    "            texts.append(segment)\n",
    "            labels.append(label)\n",
    "\n",
    "    else:\n",
    "        print(f\"No corresponding JSON file found for {xls_file}\")\n",
    "\n",
    "# At this point, texts and labels have your data labeled.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b7fb593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All data segments have labels.\n",
      "There are 28421 data segments labeled as 'other'.\n",
      "Label distribution: {'other': 28421, 'address': 1844, 'company': 731, 'date': 1678, 'total': 1578}\n"
     ]
    }
   ],
   "source": [
    "# 1. Check if the length of texts and labels lists are the same\n",
    "if len(texts) == len(labels):\n",
    "    print(\"All data segments have labels.\")\n",
    "else:\n",
    "    print(\"Mismatch between number of data segments and labels.\")\n",
    "\n",
    "# 2. Check for \"other\" labels\n",
    "other_count = labels.count(\"other\")\n",
    "if other_count > 0:\n",
    "    print(f\"There are {other_count} data segments labeled as 'other'.\")\n",
    "\n",
    "# 3. Verify the distribution of the labels\n",
    "label_distribution = {label: labels.count(label) for label in set(labels)}\n",
    "print(\"Label distribution:\", label_distribution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3048ea13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/fathimazajel/opt/anaconda3/lib/python3.9/site-packages (4.33.3)\n",
      "Requirement already satisfied: datasets in /Users/fathimazajel/opt/anaconda3/lib/python3.9/site-packages (2.14.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/fathimazajel/opt/anaconda3/lib/python3.9/site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/fathimazajel/opt/anaconda3/lib/python3.9/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/fathimazajel/opt/anaconda3/lib/python3.9/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /Users/fathimazajel/opt/anaconda3/lib/python3.9/site-packages (from transformers) (0.17.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/fathimazajel/opt/anaconda3/lib/python3.9/site-packages (from transformers) (0.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/fathimazajel/opt/anaconda3/lib/python3.9/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/fathimazajel/opt/anaconda3/lib/python3.9/site-packages (from transformers) (2022.3.15)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/fathimazajel/opt/anaconda3/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: requests in /Users/fathimazajel/opt/anaconda3/lib/python3.9/site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: filelock in /Users/fathimazajel/opt/anaconda3/lib/python3.9/site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /Users/fathimazajel/opt/anaconda3/lib/python3.9/site-packages (from datasets) (2023.6.0)\n",
      "Requirement already satisfied: multiprocess in /Users/fathimazajel/opt/anaconda3/lib/python3.9/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /Users/fathimazajel/opt/anaconda3/lib/python3.9/site-packages (from datasets) (13.0.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /Users/fathimazajel/opt/anaconda3/lib/python3.9/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /Users/fathimazajel/opt/anaconda3/lib/python3.9/site-packages (from datasets) (1.4.2)\n",
      "Requirement already satisfied: aiohttp in /Users/fathimazajel/opt/anaconda3/lib/python3.9/site-packages (from datasets) (3.8.1)\n",
      "Requirement already satisfied: xxhash in /Users/fathimazajel/opt/anaconda3/lib/python3.9/site-packages (from datasets) (3.3.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/fathimazajel/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/fathimazajel/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.6.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/fathimazajel/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/fathimazajel/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (5.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/fathimazajel/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/fathimazajel/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.1)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /Users/fathimazajel/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in /Users/fathimazajel/opt/anaconda3/lib/python3.9/site-packages (from async-timeout<5.0,>=4.0.0a3->aiohttp->datasets) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/fathimazajel/opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/fathimazajel/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/fathimazajel/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/fathimazajel/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/fathimazajel/opt/anaconda3/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/fathimazajel/opt/anaconda3/lib/python3.9/site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/fathimazajel/opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc124af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Tokenize the texts\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "train_encodings = tokenizer(texts, truncation=True, padding=True)\n",
    "\n",
    "# Convert labels from strings to integers\n",
    "label_dict = {\"company\": 0, \"date\": 1, \"address\": 2, \"total\": 3, \"other\": 4}\n",
    "labels = [label_dict[l] for l in labels]\n",
    "\n",
    "# Load into a dataset object\n",
    "dataset = load_dataset('pandas', data_files={'train': pd.DataFrame({'text': texts, 'labels': labels})})\n",
    "\n",
    "# Initialize model\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=len(label_dict))\n",
    "\n",
    "# Define training arguments and train\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    do_train=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_steps=10,\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset['train'],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(\"./my_model\")\n",
    "tokenizer.save_pretrained(\"./my_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4759e29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
